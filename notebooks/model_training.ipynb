{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vitta\\Desktop\\F5_Bootcamp\\G2_Youtube\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "from torch.nn import functional as F \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>IsToxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>people would take step back make case anyone e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>law enforcement trained shoot apprehend traine...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dont reckon black lives matter banners held wh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>large number people like police officers calle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arab dude absolutely right shot extra time sho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  IsToxic\n",
       "0  people would take step back make case anyone e...        0\n",
       "1  law enforcement trained shoot apprehend traine...        1\n",
       "2  dont reckon black lives matter banners held wh...        1\n",
       "3  large number people like police officers calle...        0\n",
       "4  arab dude absolutely right shot extra time sho...        0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/youtoxic_english_processed.csv\")\n",
    "df = df[[\"clean_text\", \"IsToxic\"]]\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenización\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_data(clean_text):\n",
    "    return tokenizer(\n",
    "        clean_text,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "tokenized_data = tokenize_data(df[\"clean_text\"].tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear dataset\n",
    "class ToxicDataset(Dataset):\n",
    "    def __init__(self, tokenized_data, labels, augment=False):\n",
    "        self.input_ids = tokenized_data[\"input_ids\"]\n",
    "        self.attention_mask = tokenized_data[\"attention_mask\"]\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        self.augment = augment\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "            \"labels\": self.labels[idx]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de entrenamiento y evaluación\n",
    "def train_one_epoch(model, dataloader, optimizer, device, scheduler, class_weights):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    accumulation_steps = 2\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", total=len(dataloader))\n",
    "    for i, batch in enumerate(progress_bar):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        weighted_loss = loss * class_weights[labels]\n",
    "        loss = weighted_loss.mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}', \n",
    "            'accuracy': f'{correct / total:.4f}'\n",
    "            })\n",
    "        \n",
    "    return total_loss / len(dataloader), correct / total\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            predictions = torch.argmax(outputs.logits, dim=1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += len(labels)\n",
    "        if total == 0:\n",
    "            return 0.0, 0.0\n",
    "        \n",
    "    return total_loss / len(dataloader), correct / total\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "Distribución de clases:\n",
      "IsToxic\n",
      "0    538\n",
      "1    462\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Tamaño del conjunto de entrenamiento: 800\n",
      "Tamaño del conjunto de validación: 200\n",
      "\n",
      "Distribución de clases en entrenamiento:\n",
      "IsToxic\n",
      "0    430\n",
      "1    370\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribución de clases en validación:\n",
      "IsToxic\n",
      "0    108\n",
      "1     92\n",
      "Name: count, dtype: int64\n",
      "Batch size ajustado: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\vitta\\Desktop\\F5_Bootcamp\\G2_Youtube\\.venv\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [02:00<00:00,  2.41s/it, loss=0.6538, accuracy=0.5238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7030, Train Accuracy: 0.5238\n",
      "Validation Loss: 0.6791, Validation Accuracy: 0.5750\n",
      "No se guardó el modelo - Overfitting (5.12%) > 5%\n",
      "\n",
      "Epoch 2/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [01:56<00:00,  2.33s/it, loss=0.6683, accuracy=0.5463]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6884, Train Accuracy: 0.5463\n",
      "Validation Loss: 0.6603, Validation Accuracy: 0.7000\n",
      "Warning: Posible overfitting detectado\n",
      "Learning rate reducida a 0.000010\n",
      "No se guardó el modelo - Overfitting (15.37%) > 5%\n",
      "\n",
      "Epoch 3/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [01:56<00:00,  2.32s/it, loss=0.6712, accuracy=0.6350]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6474, Train Accuracy: 0.6350\n",
      "Validation Loss: 0.6161, Validation Accuracy: 0.6450\n",
      "\n",
      "Guardado nuevo mejor modelo:\n",
      "Accuracy de validación: 0.6450\n",
      "Accuracy de entrenamiento: 0.6350\n",
      "Overfitting: 1.00%\n",
      "\n",
      "Epoch 4/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [01:55<00:00,  2.32s/it, loss=0.5914, accuracy=0.6837]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5965, Train Accuracy: 0.6837\n",
      "Validation Loss: 0.5551, Validation Accuracy: 0.7200\n",
      "\n",
      "Guardado nuevo mejor modelo:\n",
      "Accuracy de validación: 0.7200\n",
      "Accuracy de entrenamiento: 0.6837\n",
      "Overfitting: 3.63%\n",
      "\n",
      "Epoch 5/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [01:55<00:00,  2.32s/it, loss=0.3929, accuracy=0.7412]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5396, Train Accuracy: 0.7412\n",
      "Validation Loss: 0.5367, Validation Accuracy: 0.7200\n",
      "\n",
      "Epoch 6/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [01:57<00:00,  2.35s/it, loss=0.6988, accuracy=0.7550]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5056, Train Accuracy: 0.7550\n",
      "Validation Loss: 0.5144, Validation Accuracy: 0.7800\n",
      "\n",
      "Guardado nuevo mejor modelo:\n",
      "Accuracy de validación: 0.7800\n",
      "Accuracy de entrenamiento: 0.7550\n",
      "Overfitting: 2.50%\n",
      "\n",
      "Epoch 7/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [01:56<00:00,  2.33s/it, loss=0.4080, accuracy=0.7950]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4505, Train Accuracy: 0.7950\n",
      "Validation Loss: 0.4853, Validation Accuracy: 0.7950\n",
      "\n",
      "Guardado nuevo mejor modelo:\n",
      "Accuracy de validación: 0.7950\n",
      "Accuracy de entrenamiento: 0.7950\n",
      "Overfitting: 0.00%\n",
      "\n",
      "Epoch 8/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [01:57<00:00,  2.35s/it, loss=0.4560, accuracy=0.8337]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3958, Train Accuracy: 0.8337\n",
      "Validation Loss: 0.4877, Validation Accuracy: 0.7550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vitta\\AppData\\Local\\Temp\\ipykernel_24552\\906905357.py:180: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('../models/toxic_comment_model.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mejor modelo guardado:\n",
      "Accuracy de validación: 0.7950\n",
      "Accuracy de entrenamiento: 0.7950\n",
      "Overfitting: 0.00%\n",
      "\n",
      "Mejor accuracy del modelo: 0.7950\n",
      "\n",
      "Resultados finales:\n",
      "Mejor Accuracy: 0.7950\n",
      "Loss final: 0.4877\n",
      "Mejor Accuracy de entrenamiento: 0.7950\n",
      "Mejor Accuracy de validación: 0.7950\n",
      "Porcentaje de overfitting: 7.87%\n",
      "AVISO: Nivel moderado de overfitting\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Parámetros\n",
    "batch_size = 16\n",
    "num_epochs = 8\n",
    "learning_rate = 2e-5\n",
    "patience = 5\n",
    "dropout_rate = 0.2\n",
    "weight_decay = 0.01\n",
    "\n",
    "# Calcular pesos de las clases\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced', \n",
    "    classes=np.unique(df['IsToxic']),\n",
    "    y=df['IsToxic']\n",
    ")\n",
    "class_weights = torch.tensor(\n",
    "    class_weights, \n",
    "    dtype=torch.float\n",
    "    ).to(device)\n",
    "\n",
    "# Verificar la distribución de los datos\n",
    "print(\"\\nDistribución de clases:\")\n",
    "print(df['IsToxic'].value_counts())\n",
    "\n",
    "\n",
    "train_idx, val_idx = train_test_split(\n",
    "    np.arange(len(df)),\n",
    "    test_size=0.2,\n",
    "    stratify=df['IsToxic'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTamaño del conjunto de entrenamiento: {len(train_idx)}\")\n",
    "print(f\"Tamaño del conjunto de validación: {len(val_idx)}\")\n",
    "    \n",
    "# Verificar distribución de clases en cada split\n",
    "train_dist = df.iloc[train_idx]['IsToxic'].value_counts()\n",
    "val_dist = df.iloc[val_idx]['IsToxic'].value_counts()\n",
    "print(\"\\nDistribución de clases en entrenamiento:\")\n",
    "print(train_dist)\n",
    "print(\"\\nDistribución de clases en validación:\")\n",
    "print(val_dist)\n",
    "\n",
    "# Ajustar batch_size basado en el tamaño de los datasets\n",
    "actual_batch_size = min(\n",
    "    batch_size,\n",
    "    len(train_idx) // 10,\n",
    "    len(val_idx)\n",
    "    )\n",
    "actual_batch_size = max(1, actual_batch_size)  # Asegurar que no sea 0\n",
    "print(f\"Batch size ajustado: {actual_batch_size}\")\n",
    "# Crear data set \n",
    "train_dataset = ToxicDataset({\n",
    "    \"input_ids\": tokenized_data[\"input_ids\"][train_idx],\n",
    "    \"attention_mask\": tokenized_data[\"attention_mask\"][train_idx],\n",
    "    }, df[\"IsToxic\"].iloc[train_idx].values, augment=True)\n",
    "\n",
    "val_dataset = ToxicDataset({\n",
    "    \"input_ids\": tokenized_data[\"input_ids\"][val_idx],\n",
    "    \"attention_mask\": tokenized_data[\"attention_mask\"][val_idx],\n",
    "    }, df[\"IsToxic\"].iloc[val_idx].values, augment=False)\n",
    "    \n",
    "    # Verificar que los datasets no estén vacíos\n",
    "if len(train_dataset) == 0 or len(val_dataset) == 0:\n",
    "    print(\"Error: Dataset vacío\")\n",
    "\n",
    "# Crear dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=actual_batch_size, \n",
    "    shuffle=True,\n",
    "    drop_last=False\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=actual_batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "# Inicializar modelo\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=2,\n",
    "    hidden_dropout_prob=dropout_rate,\n",
    "    attention_probs_dropout_prob=dropout_rate,\n",
    "    classifier_dropout=dropout_rate,\n",
    "    hidden_act=\"gelu\"\n",
    "    ).to(device)\n",
    "\n",
    "# Optimizador\n",
    "optimizer = AdamW(\n",
    "    model.parameters(), \n",
    "    lr=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    eps=1e-8,\n",
    "    betas=(0.9, 0.999)\n",
    "    )\n",
    "# Scheduler\n",
    "num_training_steps = len(train_dataloader) * num_epochs\n",
    "num_warmup_steps = num_training_steps // 10\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "#Early stopping setup\n",
    "best_val_accuracy = 0\n",
    "patience_counter = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_accuracy = train_one_epoch(\n",
    "        model, \n",
    "        train_dataloader, \n",
    "        optimizer, \n",
    "        device, \n",
    "        scheduler,\n",
    "        class_weights\n",
    "        )\n",
    "    \n",
    "    # Evaluate\n",
    "    val_loss, val_accuracy = evaluate(model, val_dataloader, device)\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    if abs(train_accuracy - val_accuracy) > 0.1: # 10% de diferencia\n",
    "        print(\"Warning: Posible overfitting detectado\")\n",
    "        if patience_counter == 0:\n",
    "            # Reducir learning rate\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = param_group['lr'] * 0.5\n",
    "            print(f\"Learning rate reducida a {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        current_overfitting = abs(train_accuracy - val_accuracy) * 100\n",
    "        \n",
    "        if current_overfitting <= 5:  # Solo guardamos si el overfitting es menor al 5%\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_train_accuracy = train_accuracy\n",
    "            best_overfitting = current_overfitting\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Guardar el mejor modelo\n",
    "            if not os.path.exists('../models'):\n",
    "                os.makedirs('../models')\n",
    "            \n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_accuracy': train_accuracy,\n",
    "                'val_accuracy': val_accuracy,\n",
    "                'overfitting': current_overfitting,\n",
    "                'epoch': epoch\n",
    "            }, '../models/toxic_comment_model.pt')\n",
    "            \n",
    "            print(f\"\\nGuardado nuevo mejor modelo:\")\n",
    "            print(f\"Accuracy de validación: {val_accuracy:.4f}\")\n",
    "            print(f\"Accuracy de entrenamiento: {train_accuracy:.4f}\")\n",
    "            print(f\"Overfitting: {current_overfitting:.2f}%\")\n",
    "        else:\n",
    "            print(f\"No se guardó el modelo - Overfitting ({current_overfitting:.2f}%) > 5%\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "# Al final del entrenamiento (después del loop)\n",
    "try:\n",
    "    # Cargar el mejor modelo guardado\n",
    "    checkpoint = torch.load('../models/toxic_comment_model.pt')\n",
    "    print(\"\\nMejor modelo guardado:\")\n",
    "    print(f\"Accuracy de validación: {checkpoint['val_accuracy']:.4f}\")\n",
    "    print(f\"Accuracy de entrenamiento: {checkpoint['train_accuracy']:.4f}\")\n",
    "    print(f\"Overfitting: {checkpoint['overfitting']:.2f}%\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No se encontró ningún modelo con overfitting < 5%\")\n",
    "# Calcular el porcentaje de overfitting\n",
    "overfitting_percentage = abs(train_accuracy - val_accuracy) * 100\n",
    "\n",
    "print(f\"\\nMejor accuracy del modelo: {best_val_accuracy:.4f}\")\n",
    "\n",
    "# Resultados finales\n",
    "print(\"\\nResultados finales:\")\n",
    "print(f\"Mejor Accuracy: {best_val_accuracy:.4f}\")\n",
    "print(f\"Loss final: {val_loss:.4f}\")\n",
    "print(f\"Mejor Accuracy de entrenamiento: {best_train_accuracy:.4f}\")\n",
    "print(f\"Mejor Accuracy de validación: {best_val_accuracy:.4f}\")\n",
    "print(f\"Porcentaje de overfitting: {overfitting_percentage:.2f}%\")\n",
    "\n",
    "# Evaluación del overfitting\n",
    "if overfitting_percentage > 10:\n",
    "    print(\"WARNING: Alto nivel de overfitting detectado!\")\n",
    "elif overfitting_percentage > 5:\n",
    "    print(\"AVISO: Nivel moderado de overfitting\")\n",
    "else:\n",
    "    print(\"BIEN: Nivel de overfitting aceptable\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vitta\\AppData\\Local\\Temp\\ipykernel_24552\\406936604.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('../models/toxic_comment_model.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modelo cargado exitosamente\n",
      "Accuracy de validación: 0.7950\n",
      "Accuracy de entrenamiento: 0.7950\n",
      "Overfitting: 0.00%\n",
      "Tokenizer guardado exitosamente\n",
      "Tokenizer verificado en: ../models/toxic_comment_tokenizer\n"
     ]
    }
   ],
   "source": [
    "# Cargar el modelo guardado\n",
    "try:\n",
    "    checkpoint = torch.load('../models/toxic_comment_model.pt')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"\\nModelo cargado exitosamente\")\n",
    "    print(f\"Accuracy de validación: {checkpoint['val_accuracy']:.4f}\")\n",
    "    print(f\"Accuracy de entrenamiento: {checkpoint['train_accuracy']:.4f}\")\n",
    "    print(f\"Overfitting: {checkpoint['overfitting']:.2f}%\")\n",
    "    \n",
    "    # Guardar solo el tokenizer\n",
    "    tokenizer.save_pretrained('../models/toxic_comment_tokenizer')\n",
    "    print(\"Tokenizer guardado exitosamente\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Verificar que el tokenizer se guardó correctamente\n",
    "if os.path.exists('../models/toxic_comment_tokenizer'):\n",
    "    print(\"Tokenizer verificado en: ../models/toxic_comment_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vitta\\AppData\\Local\\Temp\\ipykernel_24552\\1274574373.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('../models/toxic_comment_model.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados para copiar al README:\n",
      "Accuracy de validación: 0.7950\n",
      "Accuracy de entrenamiento: 0.7950\n",
      "Overfitting: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Cargar el checkpoint para ver los resultados\n",
    "checkpoint = torch.load('../models/toxic_comment_model.pt')\n",
    "\n",
    "print(\"\\nResultados para copiar al README:\")\n",
    "print(f\"Accuracy de validación: {checkpoint['val_accuracy']:.4f}\")\n",
    "print(f\"Accuracy de entrenamiento: {checkpoint['train_accuracy']:.4f}\")\n",
    "print(f\"Overfitting: {checkpoint['overfitting']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vitta\\AppData\\Local\\Temp\\ipykernel_24552\\167465444.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('../models/toxic_comment_model.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mejor modelo cargado con:\n",
      "- Accuracy de validación: 0.7950\n",
      "- Accuracy de entrenamiento: 0.7950\n",
      "- Overfitting: 0.00%\n",
      "\n",
      "Archivos guardados en model-huggin:\n",
      "- config.json\n",
      "- model.safetensors\n",
      "- special_tokens_map.json\n",
      "- tokenizer_config.json\n",
      "- vocab.txt\n",
      "\n",
      "✅ Modelo y tokenizer guardados correctamente en formato Hugging Face\n",
      "Ahora puedes subir estos archivos a tu repositorio de Hugging Face\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Crear el directorio si no existe\n",
    "if not os.path.exists('../model-huggin'):\n",
    "    os.makedirs('../model-huggin')\n",
    "\n",
    "try:\n",
    "    # Cargar el mejor modelo guardado anteriormente\n",
    "    checkpoint = torch.load('../models/toxic_comment_model.pt')\n",
    "\n",
    "    # Cargar el estado del modelo\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"\\nMejor modelo cargado con:\")\n",
    "    print(f\"- Accuracy de validación: {checkpoint['val_accuracy']:.4f}\")\n",
    "    print(f\"- Accuracy de entrenamiento: {checkpoint['train_accuracy']:.4f}\")\n",
    "    print(f\"- Overfitting: {checkpoint['overfitting']:.2f}%\")\n",
    "\n",
    "\n",
    "    # Guardar en formato Hugging Face\n",
    "    model.save_pretrained('../model-huggin')\n",
    "    tokenizer.save_pretrained('../model-huggin')\n",
    "    \n",
    "    # Verificar los archivos guardados\n",
    "    files = os.listdir('../model-huggin')\n",
    "    print(\"\\nArchivos guardados en model-huggin:\")\n",
    "    for file in files:\n",
    "        print(f\"- {file}\")\n",
    "    \n",
    "    print(\"\\n✅ Modelo y tokenizer guardados correctamente en formato Hugging Face\")\n",
    "    print(\"Ahora puedes subir estos archivos a tu repositorio de Hugging Face\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
