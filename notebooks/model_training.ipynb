{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup, RobertaForSequenceClassification\n",
    "from sklearn.model_selection import KFold, StratifiedGroupKFold, train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "from torch.nn import functional as F \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>IsToxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>people would take step back make case anyone e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>law enforcement trained shoot apprehend traine...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dont reckon black lives matter banners held wh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>large number people like police officers calle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arab dude absolutely right shot extra time sho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  IsToxic\n",
       "0  people would take step back make case anyone e...        0\n",
       "1  law enforcement trained shoot apprehend traine...        1\n",
       "2  dont reckon black lives matter banners held wh...        1\n",
       "3  large number people like police officers calle...        0\n",
       "4  arab dude absolutely right shot extra time sho...        0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/youtoxic_english_processed.csv\")\n",
    "df = df[[\"clean_text\", \"IsToxic\"]]\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenización\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_data(clean_text):\n",
    "    return tokenizer(\n",
    "        clean_text,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "tokenized_data = tokenize_data(df[\"clean_text\"].tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear dataset\n",
    "class ToxicDataset(Dataset):\n",
    "    def __init__(self, tokenized_data, labels, augment=False):\n",
    "        self.input_ids = tokenized_data[\"input_ids\"]\n",
    "        self.attention_mask = tokenized_data[\"attention_mask\"]\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        self.augment = augment\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "            \"labels\": self.labels[idx]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de entrenamiento y evaluación\n",
    "def train_one_epoch(model, dataloader, optimizer, device, scheduler, class_weights):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    accumulation_steps = 2\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", total=len(dataloader))\n",
    "    for i, batch in enumerate(progress_bar):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        weighted_loss = loss * class_weights[labels]\n",
    "        loss = weighted_loss.mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}', \n",
    "            'accuracy': f'{correct / total:.4f}'\n",
    "            })\n",
    "        \n",
    "    return total_loss / len(dataloader), correct / total\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            predictions = torch.argmax(outputs.logits, dim=1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += len(labels)\n",
    "        if total == 0:\n",
    "            return 0.0, 0.0\n",
    "        \n",
    "    return total_loss / len(dataloader), correct / total\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "Distribución de clases:\n",
      "IsToxic\n",
      "0    538\n",
      "1    462\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Tamaño del conjunto de entrenamiento: 800\n",
      "Tamaño del conjunto de validación: 200\n",
      "\n",
      "Distribución de clases en entrenamiento:\n",
      "IsToxic\n",
      "0    430\n",
      "1    370\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribución de clases en validación:\n",
      "IsToxic\n",
      "0    108\n",
      "1     92\n",
      "Name: count, dtype: int64\n",
      "Batch size ajustado: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\vitta\\Desktop\\F5_Bootcamp\\G2_Youtube\\.venv\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [02:00<00:00,  2.41s/it, loss=0.6432, accuracy=0.5238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6955, Train Accuracy: 0.5238\n",
      "Validation Loss: 0.6860, Validation Accuracy: 0.5250\n",
      "\n",
      "Guardado nuevo mejor modelo:\n",
      "Accuracy de validación: 0.5250\n",
      "Accuracy de entrenamiento: 0.5238\n",
      "Overfitting: 0.12%\n",
      "\n",
      "Epoch 2/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [01:57<00:00,  2.36s/it, loss=0.7407, accuracy=0.5425]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6875, Train Accuracy: 0.5425\n",
      "Validation Loss: 0.6627, Validation Accuracy: 0.7200\n",
      "Warning: Posible overfitting detectado\n",
      "Learning rate reducida a 0.000010\n",
      "No se guardó el modelo - Overfitting (17.75%) > 5%\n",
      "\n",
      "Epoch 3/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [01:56<00:00,  2.33s/it, loss=0.5806, accuracy=0.6238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6494, Train Accuracy: 0.6238\n",
      "Validation Loss: 0.6030, Validation Accuracy: 0.7700\n",
      "Warning: Posible overfitting detectado\n",
      "Learning rate reducida a 0.000009\n",
      "No se guardó el modelo - Overfitting (14.62%) > 5%\n",
      "\n",
      "Epoch 4/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [01:54<00:00,  2.30s/it, loss=0.5852, accuracy=0.7100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5971, Train Accuracy: 0.7100\n",
      "Validation Loss: 0.5460, Validation Accuracy: 0.7450\n",
      "\n",
      "Guardado nuevo mejor modelo:\n",
      "Accuracy de validación: 0.7450\n",
      "Accuracy de entrenamiento: 0.7100\n",
      "Overfitting: 3.50%\n",
      "\n",
      "Epoch 5/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [01:58<00:00,  2.37s/it, loss=0.3825, accuracy=0.7612]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5142, Train Accuracy: 0.7612\n",
      "Validation Loss: 0.5065, Validation Accuracy: 0.7800\n",
      "\n",
      "Guardado nuevo mejor modelo:\n",
      "Accuracy de validación: 0.7800\n",
      "Accuracy de entrenamiento: 0.7612\n",
      "Overfitting: 1.88%\n",
      "\n",
      "Epoch 6/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [01:56<00:00,  2.34s/it, loss=0.2996, accuracy=0.7562]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4835, Train Accuracy: 0.7562\n",
      "Validation Loss: 0.5333, Validation Accuracy: 0.7450\n",
      "\n",
      "Epoch 7/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [01:56<00:00,  2.33s/it, loss=0.2532, accuracy=0.8125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4293, Train Accuracy: 0.8125\n",
      "Validation Loss: 0.4971, Validation Accuracy: 0.7800\n",
      "\n",
      "Epoch 8/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [01:55<00:00,  2.31s/it, loss=0.4163, accuracy=0.8425]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3654, Train Accuracy: 0.8425\n",
      "Validation Loss: 0.5292, Validation Accuracy: 0.7550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vitta\\AppData\\Local\\Temp\\ipykernel_42524\\4243134672.py:180: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('../models/toxic_comment_model.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mejor modelo guardado:\n",
      "Accuracy de validación: 0.7800\n",
      "Accuracy de entrenamiento: 0.7612\n",
      "Overfitting: 1.88%\n",
      "\n",
      "Mejor accuracy del modelo: 0.7800\n",
      "\n",
      "Resultados finales:\n",
      "Mejor Accuracy: 0.7800\n",
      "Loss final: 0.5292\n",
      "Mejor Accuracy de entrenamiento: 0.7612\n",
      "Mejor Accuracy de validación: 0.7800\n",
      "Porcentaje de overfitting: 8.75%\n",
      "AVISO: Nivel moderado de overfitting\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento con cross validation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Parámetros\n",
    "batch_size = 16\n",
    "num_epochs = 8\n",
    "learning_rate = 2e-5\n",
    "patience = 5\n",
    "dropout_rate = 0.2\n",
    "weight_decay = 0.01\n",
    "\n",
    "# Calcular pesos de las clases\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced', \n",
    "    classes=np.unique(df['IsToxic']),\n",
    "    y=df['IsToxic']\n",
    ")\n",
    "class_weights = torch.tensor(\n",
    "    class_weights, \n",
    "    dtype=torch.float\n",
    "    ).to(device)\n",
    "\n",
    "# Verificar la distribución de los datos\n",
    "print(\"\\nDistribución de clases:\")\n",
    "print(df['IsToxic'].value_counts())\n",
    "\n",
    "# Preparamos K-Fold\n",
    "train_idx, val_idx = train_test_split(\n",
    "    np.arange(len(df)),\n",
    "    test_size=0.2,\n",
    "    stratify=df['IsToxic'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTamaño del conjunto de entrenamiento: {len(train_idx)}\")\n",
    "print(f\"Tamaño del conjunto de validación: {len(val_idx)}\")\n",
    "    \n",
    "# Verificar distribución de clases en cada split\n",
    "train_dist = df.iloc[train_idx]['IsToxic'].value_counts()\n",
    "val_dist = df.iloc[val_idx]['IsToxic'].value_counts()\n",
    "print(\"\\nDistribución de clases en entrenamiento:\")\n",
    "print(train_dist)\n",
    "print(\"\\nDistribución de clases en validación:\")\n",
    "print(val_dist)\n",
    "\n",
    "# Ajustar batch_size basado en el tamaño de los datasets\n",
    "actual_batch_size = min(\n",
    "    batch_size,\n",
    "    len(train_idx) // 10,\n",
    "    len(val_idx)\n",
    "    )\n",
    "actual_batch_size = max(1, actual_batch_size)  # Asegurar que no sea 0\n",
    "print(f\"Batch size ajustado: {actual_batch_size}\")\n",
    "# Crear data set para este fold\n",
    "train_dataset = ToxicDataset({\n",
    "    \"input_ids\": tokenized_data[\"input_ids\"][train_idx],\n",
    "    \"attention_mask\": tokenized_data[\"attention_mask\"][train_idx],\n",
    "    }, df[\"IsToxic\"].iloc[train_idx].values, augment=True)\n",
    "\n",
    "val_dataset = ToxicDataset({\n",
    "    \"input_ids\": tokenized_data[\"input_ids\"][val_idx],\n",
    "    \"attention_mask\": tokenized_data[\"attention_mask\"][val_idx],\n",
    "    }, df[\"IsToxic\"].iloc[val_idx].values, augment=False)\n",
    "    \n",
    "    # Verificar que los datasets no estén vacíos\n",
    "if len(train_dataset) == 0 or len(val_dataset) == 0:\n",
    "    print(\"Error: Dataset vacío\")\n",
    "\n",
    "# Crear dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=actual_batch_size, \n",
    "    shuffle=True,\n",
    "    drop_last=False\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=actual_batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "# Inicializar modelo para cada fold\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=2,\n",
    "    hidden_dropout_prob=dropout_rate,\n",
    "    attention_probs_dropout_prob=dropout_rate,\n",
    "    classifier_dropout=dropout_rate,\n",
    "    hidden_act=\"gelu\"\n",
    "    ).to(device)\n",
    "\n",
    "# Optimizador\n",
    "optimizer = AdamW(\n",
    "    model.parameters(), \n",
    "    lr=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    eps=1e-8,\n",
    "    betas=(0.9, 0.999)\n",
    "    )\n",
    "# Scheduler\n",
    "num_training_steps = len(train_dataloader) * num_epochs\n",
    "num_warmup_steps = num_training_steps // 10\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "#Early stopping setup\n",
    "best_val_accuracy = 0\n",
    "patience_counter = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_accuracy = train_one_epoch(\n",
    "        model, \n",
    "        train_dataloader, \n",
    "        optimizer, \n",
    "        device, \n",
    "        scheduler,\n",
    "        class_weights\n",
    "        )\n",
    "    \n",
    "    # Evaluate\n",
    "    val_loss, val_accuracy = evaluate(model, val_dataloader, device)\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    if abs(train_accuracy - val_accuracy) > 0.1: # 10% de diferencia\n",
    "        print(\"Warning: Posible overfitting detectado\")\n",
    "        if patience_counter == 0:\n",
    "            # Reducir learning rate\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = param_group['lr'] * 0.5\n",
    "            print(f\"Learning rate reducida a {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        current_overfitting = abs(train_accuracy - val_accuracy) * 100\n",
    "        \n",
    "        if current_overfitting <= 5:  # Solo guardamos si el overfitting es menor al 5%\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_train_accuracy = train_accuracy\n",
    "            best_overfitting = current_overfitting\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Guardar el mejor modelo\n",
    "            if not os.path.exists('../models'):\n",
    "                os.makedirs('../models')\n",
    "            \n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_accuracy': train_accuracy,\n",
    "                'val_accuracy': val_accuracy,\n",
    "                'overfitting': current_overfitting,\n",
    "                'epoch': epoch\n",
    "            }, '../models/toxic_comment_model.pt')\n",
    "            \n",
    "            print(f\"\\nGuardado nuevo mejor modelo:\")\n",
    "            print(f\"Accuracy de validación: {val_accuracy:.4f}\")\n",
    "            print(f\"Accuracy de entrenamiento: {train_accuracy:.4f}\")\n",
    "            print(f\"Overfitting: {current_overfitting:.2f}%\")\n",
    "        else:\n",
    "            print(f\"No se guardó el modelo - Overfitting ({current_overfitting:.2f}%) > 5%\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "# Al final del entrenamiento (después del loop)\n",
    "try:\n",
    "    # Cargar el mejor modelo guardado\n",
    "    checkpoint = torch.load('../models/toxic_comment_model.pt')\n",
    "    print(\"\\nMejor modelo guardado:\")\n",
    "    print(f\"Accuracy de validación: {checkpoint['val_accuracy']:.4f}\")\n",
    "    print(f\"Accuracy de entrenamiento: {checkpoint['train_accuracy']:.4f}\")\n",
    "    print(f\"Overfitting: {checkpoint['overfitting']:.2f}%\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No se encontró ningún modelo con overfitting < 5%\")\n",
    "# Calcular el porcentaje de overfitting\n",
    "overfitting_percentage = abs(train_accuracy - val_accuracy) * 100\n",
    "\n",
    "print(f\"\\nMejor accuracy del modelo: {best_val_accuracy:.4f}\")\n",
    "\n",
    "# Resultados finales\n",
    "print(\"\\nResultados finales:\")\n",
    "print(f\"Mejor Accuracy: {best_val_accuracy:.4f}\")\n",
    "print(f\"Loss final: {val_loss:.4f}\")\n",
    "print(f\"Mejor Accuracy de entrenamiento: {best_train_accuracy:.4f}\")\n",
    "print(f\"Mejor Accuracy de validación: {best_val_accuracy:.4f}\")\n",
    "print(f\"Porcentaje de overfitting: {overfitting_percentage:.2f}%\")\n",
    "\n",
    "# Evaluación del overfitting\n",
    "if overfitting_percentage > 10:\n",
    "    print(\"WARNING: Alto nivel de overfitting detectado!\")\n",
    "elif overfitting_percentage > 5:\n",
    "    print(\"AVISO: Nivel moderado de overfitting\")\n",
    "else:\n",
    "    print(\"BIEN: Nivel de overfitting aceptable\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor accuracy alcanzado: 0.7350\n",
      "\n",
      "No se guardó el modelo porque el overfitting (10.13%) es mayor al 5%\n",
      "Considera reentrenar el modelo con diferentes hiperparámetros\n"
     ]
    }
   ],
   "source": [
    "# Guardar el mejor modelo\n",
    "print(f\"Mejor accuracy alcanzado: {best_val_accuracy:.4f}\")\n",
    "\n",
    "# Asegurarnos de que el directorio existe\n",
    "if not os.path.exists('../models'):\n",
    "    os.makedirs('../models')\n",
    "\n",
    "# Calcular overfitting final\n",
    "final_overfitting = abs(best_train_accuracy - best_val_accuracy) * 100\n",
    "\n",
    "try:\n",
    "    if final_overfitting <= 5:  # Solo guardamos si el overfitting es menor al 5%\n",
    "        # Guardar el modelo completo en formato .pt\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_accuracy': best_train_accuracy,\n",
    "            'val_accuracy': best_val_accuracy,\n",
    "            'overfitting': final_overfitting,\n",
    "            'epoch': epoch\n",
    "        }, '../models/toxic_comment_model.pt')\n",
    "        \n",
    "        # Guardar el tokenizer\n",
    "        tokenizer.save_pretrained('../models/toxic_comment_tokenizer')\n",
    "        print(\"\\nModelo y tokenizer guardados exitosamente\")\n",
    "        print(f\"Accuracy de validación: {best_val_accuracy:.4f}\")\n",
    "        print(f\"Accuracy de entrenamiento: {best_train_accuracy:.4f}\")\n",
    "        print(f\"Overfitting: {final_overfitting:.2f}%\")\n",
    "    else:\n",
    "        print(f\"\\nNo se guardó el modelo porque el overfitting ({final_overfitting:.2f}%) es mayor al 5%\")\n",
    "        print(\"Considera reentrenar el modelo con diferentes hiperparámetros\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error al guardar el modelo: {e}\")\n",
    "\n",
    "# Verificar que se puede cargar el modelo si fue guardado\n",
    "if final_overfitting <= 5:\n",
    "    try:\n",
    "        # Cargar el modelo para verificar\n",
    "        checkpoint = torch.load('../models/toxic_comment_model.pt')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(\"Modelo verificado correctamente\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al verificar el modelo: {e}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
